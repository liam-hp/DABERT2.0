Job ID: 5124
Node: node002 echo Starting: 04/23/24 14:56:16
wpatty
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Fetching hyperparameters...
	 epochs: 1000
	 test_epochs: 100
	 batch_size: 32
	 max_sentence_len: 32
Loading in data...
Setting device... cuda
Initializing config, model, and optimizer...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Beginning training on 32000 example sentences (approx. 0.17% of available)...
	 0.0% | Epoch 0 | Loss: 10.4893
	 5.0% | Epoch 50 | Loss: 8.5220
	 10.0% | Epoch 100 | Loss: 7.9944
	 15.0% | Epoch 150 | Loss: 7.8104
	 20.0% | Epoch 200 | Loss: 7.6924
	 25.0% | Epoch 250 | Loss: 8.0098
	 30.0% | Epoch 300 | Loss: 7.7560
	 35.0% | Epoch 350 | Loss: 7.6447
	 40.0% | Epoch 400 | Loss: 7.2640
	 45.0% | Epoch 450 | Loss: 7.4710
	 50.0% | Epoch 500 | Loss: 7.3403
	 55.0% | Epoch 550 | Loss: 7.0196
	 60.0% | Epoch 600 | Loss: 7.0925
	 65.0% | Epoch 650 | Loss: 6.7127
	 70.0% | Epoch 700 | Loss: 7.3929
	 75.0% | Epoch 750 | Loss: 6.9139
	 80.0% | Epoch 800 | Loss: 7.0310
	 85.0% | Epoch 850 | Loss: 7.7457
	 90.0% | Epoch 900 | Loss: 7.3015
	 95.0% | Epoch 950 | Loss: 6.9312
	 100% | Epoch 999 | Loss: 7.1723
Training finished. Total training time: 0:01:00.188263
Beginning validation on 3200 example sentences (approx. 0.02% of available)...
Validation complete. Avg validation loss: 6.98509729385376
Ending: 04/23/24 14:58:10

Job ID: 5120
Node: node002 echo Starting: 04/23/24 14:41:53
wpatty
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Fetching hyperparameters...
	 epochs: 1000
	 test_epochs: 100
	 batch_size: 32
	 max_sentence_len: 32
Loading in data...
Setting device... cuda
Initializing config, model, and optimizer...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Beginning training on 32000 example sentences (approx. 0.17% of available)...
	 0.0% Epoch 0 | Loss: 10.4493
	 5.0% Epoch 50 | Loss: 8.7041
	 10.0% Epoch 100 | Loss: 8.2139
	 20.0% Epoch 200 | Loss: 7.4413
	 40.0% Epoch 400 | Loss: 7.0902
	 80.0% Epoch 800 | Loss: 6.7359
Training finished. Total training time: 59 days, 15:51:43.747200
Beginning validation on 3200 example sentences (approx. 0.02% of available)...
Validation complete. Avg validation loss: 7.016565489768982
Ending: 04/23/24 14:43:42

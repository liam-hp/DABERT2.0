Job ID: 5911
Node: node002 echo Starting: 05/05/24 16:31:50
wpatty
‚öôÔ∏è: Fetching hyperparameters...
	 save_model_weights: True
	 save_weights_path: save-10k-6e-5-actual-custom
	 load_model_weights: True
	 load_weights_path: save-10k-2e-5-actual
	 epochs: 10000
	 test_epochs: 1000
	 batch_size: 32
	 max_sentence_len: 32
	 learning_rate: 2e-05
	 attention_type: actual
	 DNN_layers: 10
	 architectures: ['BertForMaskedLM']
	 attention_probs_dropout_prob: 0.1
	 classifier_dropout: None
	 gradient_checkpointing: False
	 hidden_act: gelu
	 hidden_dropout_prob: 0.1
	 hidden_size: 768
	 initializer_range: 0.02
	 intermediate_size: 3072
	 layer_norm_eps: 1e-12
	 max_position_embeddings: 512
	 model_type: bert
	 num_attention_heads: 12
	 num_hidden_layers: 12
	 pad_token_id: 0
	 position_embedding_type: absolute
	 transformers_version: 4.36.2
	 type_vocab_size: 2
	 use_cache: True
	 vocab_size: 30522
‚öôÔ∏è: Loading in data...
‚öôÔ∏è: Setting device... cuda
‚öôÔ∏è: Initializing config, model, and optimizer...
config PretrainedConfig {
  "DNN_layers": 10,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "attention_type": "actual",
  "batch_size": 32,
  "classifier_dropout": null,
  "epochs": 10000,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "learning_rate": 2e-05,
  "load_model_weights": true,
  "load_weights_path": "save-10k-2e-5-actual",
  "max_position_embeddings": 512,
  "max_sentence_len": 32,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "save_model_weights": true,
  "save_weights_path": "save-10k-6e-5-actual-custom",
  "test_epochs": 1000,
  "transformers_version": "4.36.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
üì∞: Beginning training on 320000 example sentences (approx. 3.68% of available)...
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
‚è≥: 	 0% | Epoch 0 | Loss: 5.1247
‚è≥: 	 5.0% | Epoch 500 | Loss: 5.2692
‚è≥: 	 10.0% | Epoch 1000 | Loss: 5.2229
‚è≥: 	 15.0% | Epoch 1500 | Loss: 5.1852
‚è≥: 	 20.0% | Epoch 2000 | Loss: 5.1562
‚è≥: 	 25.0% | Epoch 2500 | Loss: 5.1138
‚è≥: 	 30.0% | Epoch 3000 | Loss: 5.0801
‚è≥: 	 35.0% | Epoch 3500 | Loss: 4.9935
‚è≥: 	 40.0% | Epoch 4000 | Loss: 4.9937
‚è≥: 	 45.0% | Epoch 4500 | Loss: 4.9748
‚è≥: 	 50.0% | Epoch 5000 | Loss: 4.9551
‚è≥: 	 55.0% | Epoch 5500 | Loss: 4.9056
‚è≥: 	 60.0% | Epoch 6000 | Loss: 4.8511
‚è≥: 	 65.0% | Epoch 6500 | Loss: 4.7994
‚è≥: 	 70.0% | Epoch 7000 | Loss: 4.7554
‚è≥: 	 75.0% | Epoch 7500 | Loss: 4.7813
‚è≥: 	 80.0% | Epoch 8000 | Loss: 4.7650
‚è≥: 	 85.0% | Epoch 8500 | Loss: 4.7200
‚è≥: 	 90.0% | Epoch 9000 | Loss: 4.6691
‚è≥: 	 95.0% | Epoch 9500 | Loss: 4.6580
‚è≥: 	 100% | Epoch 9999 | Loss: 4.6160
üß™: Final loss: 3.8706
‚úÖ: Training finished. Total training time (H:mm:ss): 0:27:39.629235
üì∞: Beginning validation on 32000 example sentences (approx. 0.37% of available)...
Traceback (most recent call last):
  File "/storage/homes/wpatty/DABERT2.0/main.py", line 128, in <module>
    train()
  File "/storage/homes/wpatty/DABERT2.0/main.py", line 105, in train
    outputs = model(**batch) # unpack the batch dictionary directly into the model
  File "/home/wpatty/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wpatty/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/homes/wpatty/DABERT2.0/architecture.py", line 30, in forward
    return super().forward(
  File "/home/lbiester/.conda/envs/CS457/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 1380, in forward
    masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
  File "/home/wpatty/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wpatty/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wpatty/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/wpatty/.local/lib/python3.10/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 120.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 116.88 MiB is free. Process 938782 has 42.15 GiB memory in use. Including non-PyTorch memory, this process has 3.48 GiB memory in use. Process 946009 has 1.78 GiB memory in use. Of the allocated memory 2.94 GiB is allocated by PyTorch, and 235.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Ending: 05/05/24 17:00:11

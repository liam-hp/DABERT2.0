Job ID: 5137
Node: node002 echo Starting: 04/23/24 21:43:49
aballo
Invalid -W option ignored: invalid action: 'Ignore'
Fetching hyperparameters...
	 epochs: 10000
	 test_epochs: 100
	 batch_size: 32
	 max_sentence_len: 32
Loading in data...
Setting device... cuda
Initializing config, model, and optimizer...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Beginning training on 320000 example sentences (approx. 1.74% of available)...
/storage/homes/aballo/DABERT2.0/batching.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
	 0.0% | Epoch 0 | Loss: 10.5486
	 5.0% | Epoch 500 | Loss: 7.1191
	 10.0% | Epoch 1000 | Loss: 7.2521
	 15.0% | Epoch 1500 | Loss: 6.6332
	 20.0% | Epoch 2000 | Loss: 6.8322
	 25.0% | Epoch 2500 | Loss: 6.4809
	 30.0% | Epoch 3000 | Loss: 7.0841
	 35.0% | Epoch 3500 | Loss: 6.5981
	 40.0% | Epoch 4000 | Loss: 6.4073
	 45.0% | Epoch 4500 | Loss: 6.7331
	 50.0% | Epoch 5000 | Loss: 6.7711
	 55.0% | Epoch 5500 | Loss: 6.2277
	 60.0% | Epoch 6000 | Loss: 6.4792
	 65.0% | Epoch 6500 | Loss: 6.4371
	 70.0% | Epoch 7000 | Loss: 6.0099
	 75.0% | Epoch 7500 | Loss: 6.6596
	 80.0% | Epoch 8000 | Loss: 6.8066
	 85.0% | Epoch 8500 | Loss: 6.0163
	 90.0% | Epoch 9000 | Loss: 6.6583
	 95.0% | Epoch 9500 | Loss: 6.3056
	 100% | Epoch 9999 | Loss: 7.1263
Training finished. Total training time (H:mm:ss): 0:12:00.466511
Beginning validation on 3200 example sentences (approx. 0.02% of available)...
Validation complete. Avg validation loss: 6.5268169164657595
Ending: 04/23/24 21:56:48

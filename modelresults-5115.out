Job ID: 5115
Node: node002 echo Starting: 04/23/24 13:39:48
wpatty
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Fetching hyperparameters...
	 epochs: 200
	 batches: 100
	 test_epochs: 100
	 batch_size: 32
	 max_sentence_len: 32
Loading in data...
Setting device... cuda
Initializing config, model, and optimizer...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Beginning training on 640000 example sentences (approx. 3.48% of available)...
	 Epoch 0 | Loss: 10.5347
	 Epoch 1 | Loss: 10.1605
	 Epoch 2 | Loss: 10.0326
	 Epoch 3 | Loss: 9.8892
	 Epoch 4 | Loss: 9.6324
	 Epoch 5 | Loss: 9.5027
	 Epoch 6 | Loss: 9.4482
	 Epoch 7 | Loss: 9.5968
	 Epoch 8 | Loss: 8.9929
	 Epoch 9 | Loss: 9.0203
	 Epoch 10 | Loss: 9.0359
	 Epoch 11 | Loss: 9.1749
	 Epoch 12 | Loss: 9.2167
	 Epoch 13 | Loss: 8.9070
	 Epoch 14 | Loss: 9.2046
	 Epoch 15 | Loss: 8.9867
	 Epoch 16 | Loss: 9.3359
	 Epoch 17 | Loss: 9.1603
	 Epoch 18 | Loss: 9.0828
	 Epoch 19 | Loss: 9.0285
	 Epoch 20 | Loss: 8.9619
	 Epoch 21 | Loss: 9.2694
	 Epoch 22 | Loss: 9.0724
	 Epoch 23 | Loss: 8.6818
	 Epoch 24 | Loss: 8.9715
	 Epoch 25 | Loss: 8.9013
	 Epoch 26 | Loss: 8.7000
	 Epoch 27 | Loss: 8.6527
	 Epoch 28 | Loss: 8.9002
	 Epoch 29 | Loss: 8.8235
	 Epoch 30 | Loss: 8.6397
	 Epoch 31 | Loss: 8.4789
	 Epoch 32 | Loss: 8.6676
	 Epoch 33 | Loss: 9.0007
	 Epoch 34 | Loss: 8.3044
	 Epoch 35 | Loss: 8.8954
	 Epoch 36 | Loss: 8.8651
	 Epoch 37 | Loss: 9.0163
	 Epoch 38 | Loss: 8.3955
	 Epoch 39 | Loss: 8.3877
	 Epoch 40 | Loss: 8.7815
	 Epoch 41 | Loss: 8.6865
	 Epoch 42 | Loss: 8.4714
	 Epoch 43 | Loss: 7.9590
	 Epoch 44 | Loss: 8.2652
	 Epoch 45 | Loss: 8.5827
	 Epoch 46 | Loss: 8.3646
	 Epoch 47 | Loss: 8.2411
	 Epoch 48 | Loss: 8.5290
	 Epoch 49 | Loss: 7.7872
	 Epoch 50 | Loss: 8.8886
	 Epoch 51 | Loss: 8.2324
	 Epoch 52 | Loss: 8.4049
	 Epoch 53 | Loss: 8.2018
	 Epoch 54 | Loss: 7.8919
	 Epoch 55 | Loss: 8.2217
	 Epoch 56 | Loss: 8.4483
	 Epoch 57 | Loss: 8.3865
	 Epoch 58 | Loss: 8.5171
	 Epoch 59 | Loss: 7.6677
	 Epoch 60 | Loss: 8.1049
	 Epoch 61 | Loss: 8.1068
	 Epoch 62 | Loss: 8.2772
	 Epoch 63 | Loss: 8.5866
	 Epoch 64 | Loss: 7.8806
	 Epoch 65 | Loss: 7.9288
	 Epoch 66 | Loss: 7.9033
	 Epoch 67 | Loss: 8.1423
	 Epoch 68 | Loss: 8.0115
	 Epoch 69 | Loss: 8.3096
	 Epoch 70 | Loss: 7.8355
	 Epoch 71 | Loss: 7.9866
	 Epoch 72 | Loss: 8.0279
	 Epoch 73 | Loss: 8.1167
	 Epoch 74 | Loss: 8.6065
	 Epoch 75 | Loss: 8.1968
	 Epoch 76 | Loss: 8.0816
	 Epoch 77 | Loss: 7.5631
	 Epoch 78 | Loss: 8.4321
	 Epoch 79 | Loss: 8.2005
	 Epoch 80 | Loss: 8.2249
	 Epoch 81 | Loss: 7.6178
	 Epoch 82 | Loss: 7.6943
	 Epoch 83 | Loss: 8.1437
	 Epoch 84 | Loss: 7.9550
	 Epoch 85 | Loss: 8.1293
	 Epoch 86 | Loss: 7.6106
	 Epoch 87 | Loss: 8.2532
	 Epoch 88 | Loss: 7.7893
	 Epoch 89 | Loss: 7.6629
	 Epoch 90 | Loss: 8.0562
	 Epoch 91 | Loss: 8.0355
	 Epoch 92 | Loss: 7.9291
	 Epoch 93 | Loss: 7.5842
	 Epoch 94 | Loss: 7.8931
	 Epoch 95 | Loss: 7.8859
	 Epoch 96 | Loss: 7.8176
	 Epoch 97 | Loss: 7.9912
	 Epoch 98 | Loss: 7.9328
	 Epoch 99 | Loss: 7.9847
	 Epoch 100 | Loss: 7.5467
	 Epoch 101 | Loss: 7.7065
	 Epoch 102 | Loss: 7.4362
	 Epoch 103 | Loss: 7.0712
	 Epoch 104 | Loss: 7.7336
	 Epoch 105 | Loss: 7.2167
	 Epoch 106 | Loss: 7.7013
	 Epoch 107 | Loss: 8.4292
	 Epoch 108 | Loss: 7.9588
	 Epoch 109 | Loss: 7.8158
	 Epoch 110 | Loss: 7.6793
	 Epoch 111 | Loss: 7.9119
	 Epoch 112 | Loss: 7.2838
	 Epoch 113 | Loss: 7.8250
	 Epoch 114 | Loss: 7.7061
	 Epoch 115 | Loss: 7.5407
	 Epoch 116 | Loss: 7.5872
	 Epoch 117 | Loss: 7.9196
	 Epoch 118 | Loss: 7.6970
	 Epoch 119 | Loss: 7.6472
	 Epoch 120 | Loss: 7.1626
	 Epoch 121 | Loss: 7.6919
	 Epoch 122 | Loss: 7.4753
	 Epoch 123 | Loss: 7.5641
	 Epoch 124 | Loss: 7.6462
	 Epoch 125 | Loss: 7.1313
	 Epoch 126 | Loss: 7.4278
	 Epoch 127 | Loss: 7.1743
	 Epoch 128 | Loss: 7.4929
	 Epoch 129 | Loss: 7.4652
	 Epoch 130 | Loss: 7.3490
	 Epoch 131 | Loss: 7.6061
	 Epoch 132 | Loss: 7.7111
	 Epoch 133 | Loss: 7.4666
	 Epoch 134 | Loss: 7.0751
	 Epoch 135 | Loss: 7.6214
	 Epoch 136 | Loss: 7.5444
	 Epoch 137 | Loss: 7.9588
	 Epoch 138 | Loss: 7.5224
	 Epoch 139 | Loss: 7.4600
	 Epoch 140 | Loss: 7.4974
	 Epoch 141 | Loss: 7.4748
	 Epoch 142 | Loss: 7.2959
	 Epoch 143 | Loss: 7.8188
	 Epoch 144 | Loss: 7.4322
	 Epoch 145 | Loss: 7.3585
	 Epoch 146 | Loss: 7.3403
	 Epoch 147 | Loss: 6.9101
	 Epoch 148 | Loss: 7.2461
	 Epoch 149 | Loss: 7.5090
	 Epoch 150 | Loss: 7.9390
	 Epoch 151 | Loss: 7.5795
	 Epoch 152 | Loss: 7.5592
	 Epoch 153 | Loss: 6.9855
	 Epoch 154 | Loss: 7.0827
	 Epoch 155 | Loss: 6.8187
	 Epoch 156 | Loss: 7.2815
	 Epoch 157 | Loss: 7.7071
	 Epoch 158 | Loss: 7.5420
	 Epoch 159 | Loss: 7.3224
	 Epoch 160 | Loss: 7.4167
	 Epoch 161 | Loss: 7.6425
	 Epoch 162 | Loss: 7.5124
	 Epoch 163 | Loss: 7.4652
	 Epoch 164 | Loss: 7.3515
	 Epoch 165 | Loss: 7.3728
	 Epoch 166 | Loss: 7.0603
	 Epoch 167 | Loss: 6.9838
	 Epoch 168 | Loss: 6.9066
	 Epoch 169 | Loss: 7.0413
	 Epoch 170 | Loss: 7.2509
	 Epoch 171 | Loss: 6.9022
	 Epoch 172 | Loss: 7.3910
	 Epoch 173 | Loss: 7.1028
	 Epoch 174 | Loss: 7.4839
	 Epoch 175 | Loss: 7.2361
	 Epoch 176 | Loss: 7.2718
	 Epoch 177 | Loss: 7.2256
	 Epoch 178 | Loss: 6.4574
	 Epoch 179 | Loss: 7.0539
	 Epoch 180 | Loss: 7.0248
	 Epoch 181 | Loss: 7.0206
	 Epoch 182 | Loss: 6.6138
	 Epoch 183 | Loss: 7.2652
	 Epoch 184 | Loss: 7.5636
	 Epoch 185 | Loss: 6.7004
	 Epoch 186 | Loss: 6.6481
	 Epoch 187 | Loss: 7.7062
	 Epoch 188 | Loss: 6.8028
	 Epoch 189 | Loss: 6.9511
	 Epoch 190 | Loss: 7.4664
	 Epoch 191 | Loss: 7.4808
	 Epoch 192 | Loss: 7.2689
	 Epoch 193 | Loss: 6.5220
	 Epoch 194 | Loss: 7.4428
	 Epoch 195 | Loss: 7.0008
	 Epoch 196 | Loss: 7.1487
	 Epoch 197 | Loss: 6.8043
	 Epoch 198 | Loss: 6.8570
	 Epoch 199 | Loss: 6.9633
Training finished. Total training time: 1010 days, 3:14:13.891200
Beginning validation on 3200 example sentences (approx. 0.02% of available)...
Validation complete. Avg validation loss: 7.034788899421692
Ending: 04/23/24 13:57:33

Job ID: 5126
Node: node002 echo Starting: 04/23/24 15:00:12
wpatty
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Fetching hyperparameters...
	 epochs: 5000
	 test_epochs: 100
	 batch_size: 32
	 max_sentence_len: 32
Loading in data...
Setting device... cuda
Initializing config, model, and optimizer...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Beginning training on 160000 example sentences (approx. 0.87% of available)...
	 0.0% | Epoch 0 | Loss: 10.5263
	 5.0% | Epoch 250 | Loss: 7.4131
	 10.0% | Epoch 500 | Loss: 7.4024
	 15.0% | Epoch 750 | Loss: 6.7521
	 20.0% | Epoch 1000 | Loss: 6.6017
	 25.0% | Epoch 1250 | Loss: 7.0785
	 30.0% | Epoch 1500 | Loss: 7.0966
	 35.0% | Epoch 1750 | Loss: 6.5529
	 40.0% | Epoch 2000 | Loss: 6.8438
	 45.0% | Epoch 2250 | Loss: 6.7886
	 50.0% | Epoch 2500 | Loss: 7.0612
	 55.0% | Epoch 2750 | Loss: 7.0405
	 60.0% | Epoch 3000 | Loss: 7.2023
	 65.0% | Epoch 3250 | Loss: 6.6295
	 70.0% | Epoch 3500 | Loss: 6.8237
	 75.0% | Epoch 3750 | Loss: 6.8555
	 80.0% | Epoch 4000 | Loss: 7.2547
	 85.0% | Epoch 4250 | Loss: 6.5693
	 90.0% | Epoch 4500 | Loss: 6.9945
	 95.0% | Epoch 4750 | Loss: 7.1640
	 100% | Epoch 4999 | Loss: 6.4404
Training finished. Total training time (H:mm:ss): 0:05:03.440178
Beginning validation on 3200 example sentences (approx. 0.02% of available)...
Validation complete. Avg validation loss: 6.722813277244568
Ending: 04/23/24 15:06:10

Job ID: 5112
Node: node002 echo Starting: 04/23/24 13:22:32
wpatty
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Fetching hyperparameters...
	 epochs: 50
	 batches: 300
	 test_epochs: 100
	 batch_size: 32
	 max_sentence_len: 32
Loading in data...
Setting device... cuda
Initializing config, model, and optimizer...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Beginning training on 480000 example sentences (approx. 2.61% of available)...
	 Epoch 0 | Loss: 10.4722
	 Epoch 1 | Loss: 10.0786
	 Epoch 2 | Loss: 10.0387
	 Epoch 3 | Loss: 9.6757
	 Epoch 4 | Loss: 9.7205
	 Epoch 5 | Loss: 9.5259
	 Epoch 6 | Loss: 9.3769
	 Epoch 7 | Loss: 9.6678
	 Epoch 8 | Loss: 9.1711
	 Epoch 9 | Loss: 9.1357
	 Epoch 10 | Loss: 9.3632
	 Epoch 11 | Loss: 9.1572
	 Epoch 12 | Loss: 9.2200
	 Epoch 13 | Loss: 8.8260
	 Epoch 14 | Loss: 9.0635
	 Epoch 15 | Loss: 9.0502
	 Epoch 16 | Loss: 9.1326
	 Epoch 17 | Loss: 8.8840
	 Epoch 18 | Loss: 8.9751
	 Epoch 19 | Loss: 9.2238
	 Epoch 20 | Loss: 8.9116
	 Epoch 21 | Loss: 8.9049
	 Epoch 22 | Loss: 8.9043
	 Epoch 23 | Loss: 8.9014
	 Epoch 24 | Loss: 9.0814
	 Epoch 25 | Loss: 9.1602
	 Epoch 26 | Loss: 8.7734
	 Epoch 27 | Loss: 8.6940
	 Epoch 28 | Loss: 8.7092
	 Epoch 29 | Loss: 8.6860
	 Epoch 30 | Loss: 8.8317
	 Epoch 31 | Loss: 8.6828
	 Epoch 32 | Loss: 8.7814
	 Epoch 33 | Loss: 8.5554
	 Epoch 34 | Loss: 8.4854
	 Epoch 35 | Loss: 8.5663
	 Epoch 36 | Loss: 8.9663
	 Epoch 37 | Loss: 8.3675
	 Epoch 38 | Loss: 8.3700
	 Epoch 39 | Loss: 8.2839
	 Epoch 40 | Loss: 8.9387
	 Epoch 41 | Loss: 8.6285
	 Epoch 42 | Loss: 8.6444
	 Epoch 43 | Loss: 8.3619
	 Epoch 44 | Loss: 8.6329
	 Epoch 45 | Loss: 8.4952
	 Epoch 46 | Loss: 8.0154
	 Epoch 47 | Loss: 8.5115
	 Epoch 48 | Loss: 8.0823
	 Epoch 49 | Loss: 8.7249
Training finished. Total training time: 752 days, 5:41:26.217600
Beginning validation on 3200 example sentences (approx. 0.02% of available)...
Validation complete. Avg validation loss: 8.337762212753296
Ending: 04/23/24 13:35:58

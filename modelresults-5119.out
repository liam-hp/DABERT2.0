Job ID: 5119
Node: node002 echo Starting: 04/23/24 14:34:44
wpatty
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Fetching hyperparameters...
	 epochs: 1000
	 test_epochs: 100
	 batch_size: 32
	 max_sentence_len: 32
Loading in data...
Setting device... cuda
Initializing config, model, and optimizer...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Beginning training on 32000 example sentences (approx. 0.17% of available)...
	 0.0% Epoch 0 | Loss: 10.4270
	 0.05% Epoch 50 | Loss: 8.8775
	 0.1% Epoch 100 | Loss: 8.0831
	 0.2% Epoch 200 | Loss: 7.4279
	 0.4% Epoch 400 | Loss: 7.1377
	 0.8% Epoch 800 | Loss: 6.7150
Training finished. Total training time: 59 days, 16:09:36.057600
Beginning validation on 3200 example sentences (approx. 0.02% of available)...
Validation complete. Avg validation loss: 7.0239549827575685
Ending: 04/23/24 14:36:33

Job ID: 5092
Node: node002 echo Starting: 04/23/24 11:41:53
wpatty
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Fetching hyperparameters...
	 epochs: 1000
	 test_epochs: 100
	 batch_size: 32
	 max_sentence_len: 32
Loading in data...
Setting device... cuda
Initializing config, model, and optimizer...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Beginning training on 32000 example sentences (approx. 0.1738828029907842% of available)...
Estimated training time: 0:04:10s
Epoch 0 | Loss: 10.5911
Epoch 100 | Loss: 8.1805
Epoch 200 | Loss: 7.6866
Epoch 300 | Loss: 7.6255
Epoch 400 | Loss: 7.5898
Epoch 500 | Loss: 7.4930
Epoch 600 | Loss: 7.0370
Epoch 700 | Loss: 7.1933
Epoch 800 | Loss: 7.2955
Epoch 900 | Loss: 7.0567
Epoch 999 | Loss: 6.9149
Training finished. Total training time: 0:01:00.350731
Beginning validation on 3200 example sentences (approx. 0.01738828029907842% of available)...
Validation complete. Avg validation loss: 7.0350356292724605
Ending: 04/23/24 11:43:45

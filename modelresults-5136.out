Job ID: 5136
Node: node002 echo Starting: 04/23/24 21:35:25
aballo
Invalid -W option ignored: invalid action: 'Ignore'
/storage/homes/aballo/DABERT2.0/batching.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Fetching hyperparameters...
	 epochs: 10000
	 test_epochs: 100
	 batch_size: 32
	 max_sentence_len: 32
Loading in data...
Setting device... cuda
Initializing config, model, and optimizer...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Beginning training on 320000 example sentences (approx. 1.74% of available)...
	 0.0% | Epoch 0 | Loss: 10.3363
	 5.0% | Epoch 500 | Loss: 6.7781
	 10.0% | Epoch 1000 | Loss: 7.1455
	 15.0% | Epoch 1500 | Loss: 6.5702
	 20.0% | Epoch 2000 | Loss: 6.7676
	 25.0% | Epoch 2500 | Loss: 7.0127
	 30.0% | Epoch 3000 | Loss: 6.8303
	 35.0% | Epoch 3500 | Loss: 6.2640
	 40.0% | Epoch 4000 | Loss: 6.7815
	 45.0% | Epoch 4500 | Loss: 5.9717
	 50.0% | Epoch 5000 | Loss: 6.9736
	 55.0% | Epoch 5500 | Loss: 6.8945
	 60.0% | Epoch 6000 | Loss: 6.4628
	 65.0% | Epoch 6500 | Loss: 6.2962
	 70.0% | Epoch 7000 | Loss: 6.5974
	 75.0% | Epoch 7500 | Loss: 6.4551
	 80.0% | Epoch 8000 | Loss: 6.4175
	 85.0% | Epoch 8500 | Loss: 7.1552
	 90.0% | Epoch 9000 | Loss: 6.8185
	 95.0% | Epoch 9500 | Loss: 6.6933
	 100% | Epoch 9999 | Loss: 7.0516
Training finished. Total training time (H:mm:ss): 0:11:57.263576
Beginning validation on 3200 example sentences (approx. 0.02% of available)...
Validation complete. Avg validation loss: 6.614508652687073
Ending: 04/23/24 21:48:19

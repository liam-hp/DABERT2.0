Job ID: 5133
Node: node002 echo Starting: 04/23/24 16:21:09
wpatty
Invalid -W option ignored: invalid action: 'Ignore'
/home/lbiester/.conda/envs/CS457/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/lbiester/.conda/envs/CS457/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/lbiester/.conda/envs/CS457/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/storage/homes/wpatty/DABERT2.0/batching.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Fetching hyperparameters...
	 epochs: 5000
	 test_epochs: 100
	 batch_size: 32
	 max_sentence_len: 32
Loading in data...
Setting device... cuda
Initializing config, model, and optimizer...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Initializing tokenizer...
Initializing dataset...
Initializing datacollator...
Initializing dataloader...
Beginning training on 160000 example sentences (approx. 0.87% of available)...
	 0.0% | Epoch 0 | Loss: 0.0422
	 5.0% | Epoch 250 | Loss: 8.1762
	 10.0% | Epoch 500 | Loss: 7.2818
	 15.0% | Epoch 750 | Loss: 7.1067
	 20.0% | Epoch 1000 | Loss: 7.0371
	 25.0% | Epoch 1250 | Loss: 6.9924
	 30.0% | Epoch 1500 | Loss: 6.9636
	 35.0% | Epoch 1750 | Loss: 6.9321
	 40.0% | Epoch 2000 | Loss: 6.9011
	 45.0% | Epoch 2250 | Loss: 6.8696
	 50.0% | Epoch 2500 | Loss: 6.8212
	 55.0% | Epoch 2750 | Loss: 6.8049
	 60.0% | Epoch 3000 | Loss: 6.8378
	 65.0% | Epoch 3250 | Loss: 6.7944
	 70.0% | Epoch 3500 | Loss: 6.7868
	 75.0% | Epoch 3750 | Loss: 6.7426
	 80.0% | Epoch 4000 | Loss: 6.7522
	 85.0% | Epoch 4250 | Loss: 6.7545
	 90.0% | Epoch 4500 | Loss: 6.7396
	 95.0% | Epoch 4750 | Loss: 6.7181
	 100% | Epoch 4999 | Loss: 1668.2263
Final loss: 6.6887
Training finished. Total training time (H:mm:ss): 0:05:02.328910
Beginning validation on 3200 example sentences (approx. 0.02% of available)...
Validation complete. Avg validation loss: 6.69231333732605
Ending: 04/23/24 16:27:01
